{
  "articles": [
    {
      "path": "index.html",
      "title": "CoRL 2021 Supplementary Website",
      "description": "A Persistent Spatial Semantic Representation for High-level Natural Language Instruction Execution\n",
      "author": [],
      "contents": "\nModel Behavior Examples\nThe following videos show our agent executing various instructions. Each example shows:\nTop-left: The input RGB image with interaction action masks overlaid during timesteps when the agent performs an interaction action. The interaction action argument mask is computed as the intersection of the two masks in middle-right and bottom-right positions.\nBottom-left: Predicted segmentation and depth used to build the semantic voxel map.\nCenter: Semantic voxel map. Different colors indicate different classes of objects. The brighter colors represent voxels that are observed in the current timestep. The more washed-out colors represent voxels remembered from previous timesteps. Agent position is represented as a black pillar. The current navigation goal is shown as a red pillar. The 3D argument mask of the current subgoal is shown in bright yellow. These three special voxel classes are not part of the semantc voxel map, but are included here for visualization only. The text overlays show the instruction input, the current subgoal, the current action, and the state of the low-level controller (either exploring or interacting).\nTop-right: Value iteration network value function in the birds-eye view. White pixels are the goal location of value 1. Black pixels are obstacles of value -1. Other pixels are free or unobserved space with varying values between 0 and 1.\nMiddle-right: Mask identifying all pixels that belong to the current subgoal argument class according to the most recent segmentation image.\nBottom-right: The 3D subgoal argument mask projected in the first-person view.\nExample 1 - Secure two discs in a bedroom safe\nResult: Success\n\n\nExample 2 - Place the sliced apple on the counter\nResult: Failure\n\n\nPredicted vs Ground-truth Segmentation and Depth\nExample 3A - Put a clean bar of soap in a cabinet\nResult: Success\nThis example uses learned monocular and depth models as required by the ALFRED benchmark.\n\n\nExample 3B (ground truth depth and segmentation) - Put a clean bar of soap in a cabinet\nResult: Failure\nThis example provides ground truth depth and segmentation images at test-time.\n\n\nPerfect depth and segmentation in Example B results in clear semantic voxel maps. As a result, the agent knows perfectly the location of every observed object, and succeeds in most interaction actions with the first attempt. However, the agent loses track of which sink the soap bar was placed in and toggles the wrong faucet resulting in task failure.\nIn Example 3A, the semantic voxel map is built from predicted monocular depth and segmentation, and as a result is a more noisy. However, it still facilitates the necessary reasoning to complete the task. In this case, the agent sometimes has to re-try a couple of times before succeeding in each interaction action due to uncertainty about locations of various objects.\n\n\n\n",
      "last_modified": "2021-07-01T20:44:10-04:00"
    }
  ],
  "collections": []
}
